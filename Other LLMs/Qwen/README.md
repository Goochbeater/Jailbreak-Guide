# Qwen

## Model Information
- **Latest Versions**:
  - Qwen3-235B-A22B-Instruct/Thinking-2507 (235B total, 22B activated)
  - Qwen3-30B-A3B-Instruct/Thinking-2507 (30B total, 3B activated)
  - Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B (dense models)
  - Qwen3-Next-80B-A3B (80B total, 3B activated - ultra efficient)
  - Qwen3-Max (non-reasoning flagship)
  - Qwen3-Coder (agentic coding model)
- **Access**:
  - https://huggingface.co/chat/
  - https://chat.qwenlm.ai/
- **Cost**: Free
- **Censorship**: 8/10 (external filtering can be very restrictive)
- **Intelligence**: 6-8/10 depending on model (QwQ/Qwen3-235B: 8/10, Instruct: 5-6/10)
- **Context**: 128K natively, 1M with extension, 256K for Qwen3-Next
- **License**: Apache 2.0

## Features
- Trained on 36 trillion tokens covering 119 languages
- Hybrid thinking/non-thinking modes in single model
- Qwen3-Next: 10x higher throughput than Qwen3-32B for long contexts
- Native vision capabilities in some variants
- Competitive with DeepSeek R1, o1, o3-mini, Grok-3
